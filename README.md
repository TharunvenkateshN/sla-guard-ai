ðŸ›¡ï¸ SLA-Guard AI

Predictive SLA Risk Monitoring with Explainable ML

ðŸš¨ Problem Statement

Service Level Agreements (SLAs) define availability guarantees such as 99.9% uptime.
However, most monitoring systems today are reactive:

SLA breaches are detected after damage is done

Engineers see metrics, not future risk

Alerts are threshold-based, noisy, and lack context

This leads to:

Late incident response

Alert fatigue

Loss of trust in monitoring systems

ðŸ’¡ Solution â€” SLA-Guard AI

SLA-Guard AI predicts SLA breaches before they happen.

Instead of reacting to failures, it answers:

â€œWill this service violate its SLA in the next X hours â€” and why?â€

It combines:

SRE-grade feature engineering

Explainable machine learning

Clear, actionable alerts

ðŸŽ¯ Core Capabilities (v1)
âœ… Predictive SLA Risk

Outputs a probability of SLA breach

Time-horizon aware (e.g., next 6 hours)

âœ… Explainability (Trust-First)

Always explains why a service is at risk

Example:

High SLA burn rate

Latency deviating from baseline

âœ… ML-Backed Decisions

Logistic Regression for:

Probability output

Interpretability

Stability

âœ… Proactive Alerts

Alerts triggered only when risk crosses threshold

Stored for auditability

ðŸ§  How It Works (High Level)
Metrics â†’ Feature Engineering â†’ ML Prediction â†’ Explanation â†’ Alert

1ï¸âƒ£ Telemetry Ingestion

Every few minutes:

Uptime %

Latency (avg, p95)

Error rate

Request volume

Deployment events

2ï¸âƒ£ Feature Engineering

Derived signals:

SLA burn rate

Error trend slope

Error acceleration

Latency deviation from baseline

3ï¸âƒ£ Risk Prediction

Logistic Regression outputs breach probability

Rule-based logic provides explanation

4ï¸âƒ£ Alerting

Alerts triggered when risk > threshold

Stored for visibility and audit

ðŸ§ª Example API Response
{
  "service": "payment-service",
  "sla_risk_probability": 0.82,
  "time_horizon": "6 hours",
  "alert_required": true,
  "top_factors": [
    "High SLA burn rate",
    "Latency deviating from baseline"
  ]
}

ðŸ—ï¸ Architecture
Backend

FastAPI â€” API layer

Supabase (PostgreSQL) â€” persistence

scikit-learn â€” ML model

joblib â€” model loading

Architecture Pattern

Offline ML training

Online inference (clean separation)

Rule-based explanation + ML probability

ðŸ“‚ Project Structure
sla-guard-ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ db/
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ sla_risk_model.joblib
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ðŸ” Trust & Reliability Philosophy

SLA-Guard AI is a trust-critical system.

Design principles:

No silent failures

Explain every decision

Prefer conservative alerts over noisy ones

ðŸ”­ Roadmap (v2 â€“ Planned)
ðŸ›¡ï¸ Self-Observability (Meta-Monitoring)

â€œWho watches the watcher?â€

Planned enhancements:

SLA-Guard AI monitors its own health

Detects degradation in prediction pipeline

Enters safe-mode if reliability is compromised

Transparent messaging to users during degraded states

This ensures:

User trust is preserved

No false confidence is ever given

ðŸš€ Why SLA-Guard AI Is Different

Existing Tools	                SLA-Guard AI

Reactive alerts                	Predictive risk
Metric-level focus	            SLA-level decisions
Threshold-based	                Learned patterns
Black-box alerts	              Explainable causes
Vendor-locked	                  Vendor-neutral

ðŸ§ª MVP Status

v1 Complete

( 1.) Predicts SLA breaches

( 2.) Explains why

( 3.) Alerts proactively

( 4.) ML-backed, explainable, and stable

ðŸ‘¤ Intended Users

-> SREs

-> Platform Engineers

-> Reliability teams

-> DevOps teams